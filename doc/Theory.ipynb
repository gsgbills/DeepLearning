{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#Theory-vs-Practice\" data-toc-modified-id=\"Theory-vs-Practice-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Theory vs Practice</a></span><ul class=\"toc-item\"><li><span><a href=\"#curse-of-dimensionality?\" data-toc-modified-id=\"curse-of-dimensionality?-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>curse of dimensionality?</a></span></li><li><span><a href=\"#No-free-lunch-theorem\" data-toc-modified-id=\"No-free-lunch-theorem-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span><a href=\"https://en.wikipedia.org/wiki/No_free_lunch_theorem\" target=\"_blank\">No free lunch theorem</a></a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory vs Practice\n",
    "\n",
    "[38:16] There are two concepts you often hear — curse of dimensionality and no free lunch theorem. They are both largely meaningless and basically stupid and yet many people in the field not only know that but think the opposite so it is well worth explaining. \n",
    "\n",
    "## curse of dimensionality? \n",
    "The curse of dimensionality is that the more columns you have, it creates a space that is more and more empty. \n",
    "The more dimensions you have, the more all of the points sit on the edge of that space. \n",
    "If you just have a single dimension where things are random, then they are spread out all over. \n",
    "If it is a square then the probability that they are in the middle means that they cannot be on the edge of either dimension so it is a little less likely that they are not on the edge. \n",
    "Each dimension we add, it becomes multiplicatively less likely that a point is not on the edge of at least one dimension.\n",
    "So in high dimensions, everything sits on the edge. \n",
    "What that means, in theory, is that the distance between points is much less meaningful. \n",
    "So if we assume it matters, then it would suggest that when we have lots of columns,\n",
    "and we just use them without being careful to remove the ones we do not care about that things will not work. This turns out not to be the case for a number of reasons.\n",
    "\n",
    "Points still do have different distances away from each other. \n",
    "Just because they are on the edge, they still vary on how far away they are from each other.\n",
    "So point A is more similar to point B than it is to point C.\n",
    "\n",
    "Algorithms like k-nearest neighbors work well in high dimensions despite what the theoreticians claimed.\n",
    "\n",
    "In the 90s, theory took over ML. \n",
    "Support Vector Machines (SVMs) were theoretically well justified, easy to analyze mathematically, \n",
    "and to prove things about them.\n",
    "And we lost (a decade?) of real practical development. \n",
    "Theories like the curse of dimensionality became very popular. \n",
    "Nowadays ML has become very empirical.\n",
    "In practice, building models on lots of columns works really well.\n",
    "\n",
    "## [No free lunch theorem](https://en.wikipedia.org/wiki/No_free_lunch_theorem)\n",
    "[41:08] — \n",
    "The NFL claim is that there is no type of model that works well for any kind of problem/dataset. \n",
    "Mathematically, any random dataset by definition is random, so there is not going to be some way of looking at every possible random dataset that is in someway more useful than any other approach. \n",
    "\n",
    "But in the real world, we look at data which is not truly random. \n",
    "Mathematically, the data \"sits on some lower dimensional manifold\". \n",
    "It was created by some causal structure, with some relationships, etc.\n",
    "So in the real world we are not using \"random\" datasets.\n",
    "Hence there are techniques that work much better than other techniques for nearly all of the datasets you look at. \n",
    "There are empirical researchers who study which techniques work a lot of the time. \n",
    "Ensembles of decisions trees, like random forests, is perhaps the technique which most often comes at the top. \n",
    "Fast.ai provides a standard way to pre-process them properly and set their parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
