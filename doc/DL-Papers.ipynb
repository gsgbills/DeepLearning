{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#Articles-Relevant-to-Deep-Learning\" data-toc-modified-id=\"Articles-Relevant-to-Deep-Learning-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Articles Relevant to Deep Learning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Deep-Learning-basics\" data-toc-modified-id=\"Deep-Learning-basics-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Deep Learning basics</a></span></li><li><span><a href=\"#Initialization\" data-toc-modified-id=\"Initialization-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Initialization</a></span></li><li><span><a href=\"#Optimization\" data-toc-modified-id=\"Optimization-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Optimization</a></span></li><li><span><a href=\"#Regularization\" data-toc-modified-id=\"Regularization-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Regularization</a></span></li><li><span><a href=\"#RNN\" data-toc-modified-id=\"RNN-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>RNN</a></span></li><li><span><a href=\"#NLP:\" data-toc-modified-id=\"NLP:-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>NLP:</a></span></li><li><span><a href=\"#Image-Enhancement:\" data-toc-modified-id=\"Image-Enhancement:-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Image Enhancement:</a></span></li><li><span><a href=\"#Parameters?\" data-toc-modified-id=\"Parameters?-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Parameters?</a></span></li><li><span><a href=\"#GRU-and-LSTM\" data-toc-modified-id=\"GRU-and-LSTM-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>GRU and LSTM</a></span></li></ul></li><li><span><a href=\"#Tools\" data-toc-modified-id=\"Tools-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Tools</a></span><ul class=\"toc-item\"><li><span><a href=\"#Google-Fire-library\" data-toc-modified-id=\"Google-Fire-library-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span><a href=\"https://github.com/google/python-fire\" target=\"_blank\">Google Fire library</a></a></span></li><li><span><a href=\"#SentencePiece\" data-toc-modified-id=\"SentencePiece-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span><a href=\"https://github.com/google/sentencepiece\" target=\"_blank\">SentencePiece</a></a></span></li></ul></li><li><span><a href=\"#Applications\" data-toc-modified-id=\"Applications-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Applications</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[WolframAlpha](https://www.wolframalpha.com/) \n",
    "\n",
    "# Articles Relevant to Deep Learning\n",
    "\n",
    "## Deep Learning basics\n",
    "- [Calculus on Computational Graphs: Backpropagation](http://colah.github.io/posts/2015-08-Backprop/)\n",
    "a great article by Chris Olah on Backpropagation as a chain rule.\n",
    "- [Structured Deep Learning](https://towardsdatascience.com/structured-deep-learning-b8ca4138b848)\n",
    "- [How do we ‘train’ neural networks ?](https://towardsdatascience.com/how-do-we-train-neural-networks-edd985562b73)\n",
    "\n",
    "## Initialization\n",
    "- [Initialization Of Deep Networks Case of Rectifiers](http://www.jefkine.com/deep/2016/08/08/initialization-of-deep-networks-case-of-rectifiers/)\n",
    "-[A Simple Way to Initialize Recurrent Networks of Rectified Linear Units](https://arxiv.org/abs/1504.00941)\n",
    "\n",
    "## Optimization\n",
    "- [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)\n",
    "- [Fixing Weight Decay Regularization in Adam](https://arxiv.org/pdf/1711.05101.pdf)\n",
    "- [Optimization for Deep Learning Highlights in 2017](http://ruder.io/deep-learning-optimization-2017/index.html)\n",
    "\n",
    "## Regularization\n",
    "[Regularizing and Optimizing LSTM Language Models](https://openreview.net/pdf?id=SyyGPP0TZ) \n",
    "Stephen Merity et al. takes different AWD-LSTM hyper-parameters, \n",
    "train different models and then use a random forest to find out the feature importance \n",
    "— which ones actually matter the most and then figure out how to set them. \n",
    "\n",
    "## RNN\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "- [Attention and Augmented Recurrent Neural Networks](https://distill.pub/2016/augmented-rnns/)\n",
    "\n",
    "## NLP:\n",
    "- [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)\n",
    "Introduced this idea of attention and other key concepts. \n",
    "Attention has been used not just for text, \n",
    "but for things like reading text out of pictures or doing various things with computer vision.\n",
    "- [Grammar as a Foreign Language](https://arxiv.org/abs/1412.7449)\n",
    "(by Geoffrey Hinton) used RNN with attention to replace rules-based grammar \n",
    "with an RNN which automatically tagged each word based on the grammar. \n",
    "It did better than any rules based system.\n",
    "- [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078)\n",
    "\n",
    "## Image Enhancement:\n",
    "- [Progressive Growing of GANS for Improved Quality, Stability, and Variation.](http://research.nvidia.com/publication/2017-10_Progressive-Growing-of). \n",
    "Progressive GANS gradually increase the image size. \n",
    "Great paper to study, built something that works well, uses many DL ideas.  \n",
    "Nvidia does not do a lot of papers, but when they do, they are good.\n",
    "- [A Neural Algorithm of Artistic Style](https://arxiv.org/abs/1508.06576)\n",
    "Original artistic style paper, well done. Since then many updates and diferent newer approaches. \n",
    "\n",
    "\n",
    "##  Parameters?\n",
    "-[Universal Language Model Fine-tuning for Text Classification](https://arxiv.org/pdf/1801.06146) \n",
    "which included the ablation studies,  \n",
    "formalized **`discriminative learning rates`** and **`Gradual unfreezing` and `concat pooling`.**\n",
    "\n",
    "-[A Disciplined Approach to Neural Network Parameters: Part I - Learning Rate, Batch Size, Momentum, and Weight Decay](https://arxiv.org/abs/1803.09820). \n",
    "Leslie Smith's paper, introduced `cyclical Learning Rates`  A great paper to read and summarize...\n",
    "\n",
    "## GRU and LSTM\n",
    "- [Understanding LSTM Networks by Colah](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools\n",
    "Google tools for NLP\n",
    "\n",
    "## [Google Fire library](https://github.com/google/python-fire) \n",
    "is great for ablation studies.\n",
    "JH put all the scripts under `dl2/imdb_scripts`. **TODO: review these scripts.**\n",
    "\n",
    "## [SentencePiece](https://github.com/google/sentencepiece) \n",
    "is useful to tokenize in sub-word units, etc.\n",
    "\"SentencePiece allows us to make a purely end-to-end system that does not depend on language-specific pre/postprocessing\". <br>\n",
    "SentencePiece is an unsupervised text tokenizer and detokenizer mainly for NN-based text generation systems where the vocabulary size is predetermined prior to the neural model training. \n",
    "SentencePiece implements:\n",
    "- subword units (e.g., byte-pair-encoding (BPE) [Sennrich et al.]) \n",
    "and \n",
    "- unigram language model [Kudo.]) with the extension of direct training from raw sentences. \n",
    "\n",
    "JH got results nearly as good as word tokenization, suspect it could be better...\n",
    "But in any case, it is good because end up with vocabulary of sub-units which is very good...\n",
    "It is a bit difficult to install, bad error messages, etc.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications\n",
    "\n",
    "[Swiftkey](https://blog.swiftkey.com/neural-networks-a-meaningful-leap-for-mobile-typing/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
