{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#Random-Forest-Basics\" data-toc-modified-id=\"Random-Forest-Basics-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Random Forest Basics</a></span><ul class=\"toc-item\"><li><span><a href=\"#Relationship-between-RF-hyperparameters-and-effect-on-overfitting.\" data-toc-modified-id=\"Relationship-between-RF-hyperparameters-and-effect-on-overfitting.-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Relationship between RF hyperparameters and effect on overfitting.</a></span><ul class=\"toc-item\"><li><span><a href=\"#set_RF_samples\" data-toc-modified-id=\"set_RF_samples-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span><code>set_RF_samples</code></a></span></li><li><span><a href=\"#min_samples_leaf.\" data-toc-modified-id=\"min_samples_leaf.-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span><code>min_samples_leaf</code>.</a></span></li><li><span><a href=\"#max_features\" data-toc-modified-id=\"max_features-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span><code>max_features</code></a></span></li><li><span><a href=\"#jobs\" data-toc-modified-id=\"jobs-1.1.4\"><span class=\"toc-item-num\">1.1.4&nbsp;&nbsp;</span><code>jobs</code></a></span></li></ul></li></ul></li><li><span><a href=\"#random-forest-interpretation\" data-toc-modified-id=\"random-forest-interpretation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>random forest interpretation</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Variable-interactions\" data-toc-modified-id=\"Variable-interactions-2.0.1\"><span class=\"toc-item-num\">2.0.1&nbsp;&nbsp;</span>Variable interactions</a></span><ul class=\"toc-item\"><li><span><a href=\"#One-variable-at-a-time?\" data-toc-modified-id=\"One-variable-at-a-time?-2.0.1.1\"><span class=\"toc-item-num\">2.0.1.1&nbsp;&nbsp;</span>One variable at a time?</a></span></li><li><span><a href=\"#Q:-what-are-the-units-of-the-importances?\" data-toc-modified-id=\"Q:-what-are-the-units-of-the-importances?-2.0.1.2\"><span class=\"toc-item-num\">2.0.1.2&nbsp;&nbsp;</span>Q: what are the units of the importances?</a></span></li></ul></li></ul></li><li><span><a href=\"#Other-(wrong)-approaches-in-academia\" data-toc-modified-id=\"Other-(wrong)-approaches-in-academia-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Other (wrong) approaches in academia</a></span></li><li><span><a href=\"#one-hot-encoding\" data-toc-modified-id=\"one-hot-encoding-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>one hot encoding</a></span><ul class=\"toc-item\"><li><span><a href=\"#Cluster-Analysis\" data-toc-modified-id=\"Cluster-Analysis-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Cluster Analysis</a></span></li></ul></li></ul></li><li><span><a href=\"#Generalization\" data-toc-modified-id=\"Generalization-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Generalization</a></span><ul class=\"toc-item\"><li><span><a href=\"#OOB-Score\" data-toc-modified-id=\"OOB-Score-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>OOB Score</a></span><ul class=\"toc-item\"><li><span><a href=\"#Kaggle-Scores\" data-toc-modified-id=\"Kaggle-Scores-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Kaggle Scores</a></span></li></ul></li><li><span><a href=\"#Random-Sample-issues\" data-toc-modified-id=\"Random-Sample-issues-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Random Sample issues</a></span><ul class=\"toc-item\"><li><span><a href=\"#Q:-What-about-selecting-the-training-set?\" data-toc-modified-id=\"Q:-What-about-selecting-the-training-set?-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Q: What about selecting the training set?</a></span></li></ul></li><li><span><a href=\"#Ensure-that-the-validation-set-is-representative-of-the-test-set\" data-toc-modified-id=\"Ensure-that-the-validation-set-is-representative-of-the-test-set-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Ensure that the validation set is representative of the test-set</a></span></li><li><span><a href=\"#Cross-validation\" data-toc-modified-id=\"Cross-validation-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Cross-validation</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Reasons-not-to-use-cross-validation:\" data-toc-modified-id=\"Reasons-not-to-use-cross-validation:-3.4.0.1\"><span class=\"toc-item-num\">3.4.0.1&nbsp;&nbsp;</span>Reasons not to use cross-validation:</a></span></li></ul></li></ul></li><li><span><a href=\"#Extrapolation\" data-toc-modified-id=\"Extrapolation-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Extrapolation</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Basics\n",
    "\n",
    "## Relationship between RF hyperparameters and effect on overfitting.\n",
    "\n",
    "### `set_RF_samples`\n",
    "Let's say the full dataset has 1M rows, and we `set_RF_samples(20000)`.\n",
    "This determines that each tree will have 200000 rows. \n",
    "Before we start a new tree, we either bootstrap a sample, sampling with replacement,\n",
    "or we pull a subsample of a smaller number of rows and then we build a tree from there.<br>\n",
    "Step one is we we grab a few rows at random from the full dataset, \n",
    "and from it we build a tree of size set_RF_samples. <br>\n",
    "Assuming the tree remains balanced, it will be $log_2 20,000$ layers deep.\n",
    "(assuming we're growing it until every leaf is of size one). <br>\n",
    "At the bottom, there would be 20K leaf nodes, as every leaf node has a single item in it.<br>\n",
    "When we decrease the sample size, there are less final decisions that can be made.\n",
    "Therefore the tree is \"less rich\" in terms of what it can predict because:\n",
    "- it's making less different individual decisions and \n",
    "- it is making less binary choices to get to those decisions.\n",
    "Therefore setting `set_RF_samples` lower means that:\n",
    "- we overfit less and  \n",
    "- we have a less accurate individual tree model.\n",
    "\n",
    "The inventor of RF described it as trying to do 2 things when we build a model with bagging:\n",
    "- each individual estimator as accurate as possible on the training set. Each model is a strong predictive model\n",
    "- across estimators the correlation between them is as low as possible. \n",
    "When we average them together they generalize.<br>\n",
    "By decreasing the `set_RF_samples` number we decrease the power of the estimator and increase the correlation.\n",
    "Is that going to result in a better or a worse validation set result?\n",
    "It depends. This is a compromise which we need to figure out. \n",
    "\n",
    "A 2nd benefit `set_RF_samples` is to run more quickly. \n",
    "On a large data set, eg 100M, it won't be possible to run it on the full dataset.\n",
    "So we either pick a subsample before start, or use `set_RF_samples`.\n",
    "\n",
    "### `min_samples_leaf`.\n",
    "If we change `min_samples_leaf` from 1 to 2, the new depth would be $(log_2 2000) - 1$.\n",
    "Each time we double `min_samples_leaf` we're removing one layer from the tree.<br>\n",
    "Now we will have 10000 leaf nodes, and each leaf node has more than one thing.\n",
    "We get a more stable average that we're calculating in each tree. <br>\n",
    "We've got less depth, less decisions to make, and a smaller number of leaf nodes.\n",
    "Each estimator would be less predictive and the estimators would be less correlated.\n",
    "This might help avoid overfitting. <br>\n",
    "Not every node will have exactly two. \n",
    "It might try to do a split if it has 100 items in a\n",
    "leaf node but they're all the same in terms of the dependent variable.\n",
    "So if we get to a leaf node where every single one of them has the same price (or every one is a dog)\n",
    "then there is no split that we can do that's going to improve the \"information\".\n",
    "\n",
    "**Information** is the term we use in RF to describe how much are we improving the model, \n",
    "the difference we create from a split.\n",
    "**\"Information gain\"** is how much better did the model get by adding an additional split point.\n",
    "It could be based on our MSC or it could be based on cross-entropy, \n",
    "or it could be based on how different to the standard deviations or whatever.\n",
    "\n",
    "### `max_features` \n",
    "We pull out our subsample or a bootstrap sample and that's kept for the whole tree, all of the columns in there.<br>\n",
    "With `max_features = 0.5` at each split we'd pick a different half of the features.\n",
    "We do that because we want the trees to be as rich as possible.\n",
    "If we were only doing a small number of trees, eg 10 trees,\n",
    "with the same column set all the way through, we're not getting much variety.\n",
    "This way, at least in theory, is going to give us a better set of trees,\n",
    "picking a different random subset of features at every decision point.<br>\n",
    "Using `max_features`, an individual tree is less accurate, but the forest more varied.\n",
    "This can be critical. \n",
    "If we've got one feature $F$ that it is so predictive that every\n",
    "random subsample always starts out by splitting on $F$,\n",
    "then the trees are very similar, as they all have the same initial split.\n",
    "But there may be other interesting initial splits that create different interactions of variables.\n",
    "If 50% of the time that feature won't even be available at the top of the tree,\n",
    "then 50% of the trees will have a different initial split.\n",
    "More variation can create more generalized trees that have less correlation with each\n",
    "other, even though the individual trees probably won't be as predictive.\n",
    "\n",
    "In practice, per little picture, as we add more trees,\n",
    "if `max_features` equals none that's going to use all the features every time.\n",
    "Then with few trees can still give a pretty good error.\n",
    "But as we create more trees it's not going to help as much,\n",
    "because they're all pretty similar, as they're all trying every single variable.\n",
    "But, if we say max_features equal square root or equals $log_2$ \n",
    "then as we add more estimators we see improvements.\n",
    "\n",
    "### `jobs`\n",
    "does not impact the training just performance.\n",
    "`jobs` is how many cpu to use, will make it faster up to a point,\n",
    "e.g., > 8 may have diminishing returns.\n",
    "-1 says use all of your cores.\n",
    "\n",
    "\n",
    "# random forest interpretation \n",
    "Confidence based on tree variance is not in any other library.\n",
    "Feature importance does is already in a lot of kernels.\n",
    "Feature importance works by randomly shuffling a column \n",
    "at a time and seeing how accurate the model is when we pass it in all the data as before,\n",
    "but with one column shuffled.\n",
    "\n",
    "### Variable interactions\n",
    "The issue of variable **interactions** is critical. \n",
    "For example, consider `yearmade`, and `saledate`.\n",
    "Obviously what matters is the combination: how old is the piece of equipment when it got sold.\n",
    "If we only included one of these we're going to underestimate how important that feature is. <br>\n",
    "We could create a simple logistic regression, which is as good as any RF, \n",
    "if we know ahead of time exactly what variables we need, how they interact, \n",
    "how they need to be transformed, etc.\n",
    "Here we could create a new field which was equal to\n",
    "`age = saleyear - yearmade` and feed it to the model.\n",
    "But we never know all interactions, we just might have a guess of how the variables interact. \n",
    "In the real-world causal structures got many things interacting in subtle ways.\n",
    "So using trees, gradient boosting machines or random forests works so well.\n",
    "\n",
    "#### One variable at a time?\n",
    "We can't figure it one variable at a time which one is most correlated with the dependent variable.\n",
    "What if all variables copied the same variable?\n",
    "Then they're all going to seem equally important but in fact it's really just one.\n",
    "If we had a column appeared twice, shuffling that column isn't going to make the model much worse.\n",
    "If we had `max_features =0.5`, some of the times we get version A of the column, others version B.\n",
    "Half the time shuffling version A (B) makes a tree a bit worse.\n",
    "It'll show that both A and B are somewhat important, and share the importance between the two features.<br>\n",
    "Having 2+ variables that are related closely to each other \n",
    "means that we will often underestimate their importance using RF.\n",
    "\n",
    "#### Q: what are the units of the importances?\n",
    "A: depends on the library we're using. \n",
    "The units of measure importance don't matter too much.\n",
    "\n",
    "In this particular library 0.005 is often a cutoff to use.\n",
    "We care about the feature importance ordered for each variable.\n",
    "Then zooming in turning into a bar plot.\n",
    "Below a threshold, they're all pretty flat, about 0.05.\n",
    "We removed them and the validation score didn't get worse.\n",
    "If it did get worse, we decrease the threshold until it doesn't get worse.\n",
    "\n",
    "One goal is to remove variables such that our score will not get worse.\n",
    "After looking at the feature importance plot, we see that < 0.005 is a long tail of little importance.\n",
    "So we try removing them. <br>\n",
    "Knowing importance helps on where to allocate feature engineering time.\n",
    "It helps in dealing with missing values is there noise and the data.\n",
    "For example if it was a high cardinality categorical variable that was originally a string,\n",
    "we could decide to split it into columns, eg \"type of vehicle\" a hyphen and then \"size of the vehicle\".\n",
    "\n",
    "After doing some feature engineering, the RMSE score got worse. \n",
    "There are 2 possible reasons why: \n",
    "1. Overfitting. if you're overfitting, the OOB also will get worse. \n",
    "If doing a huge data set with a small `set_RF_sample` so we can't use OOB.\n",
    "Then instead create a second validation set which is a random sample.\n",
    "If the OOB or your random sample validation set is much worse then we are overfitting.\n",
    "But RFs don't overfit badly, unless we use weird parameters like only one estimator.\n",
    "It can overfit but not so much to destroy the validation score by adding a variable.\n",
    "It's easy to check: the OOB score or your random sample validation score hasn't got worse.\n",
    "\n",
    "2. Doing something that is true in the training set but not true in the validation set.\n",
    "This can only happen when the validation set is not a random sample.\n",
    "For example, we've intentionally made a validation set that's for a\n",
    "different date range, i.e., for the most recent two weeks.\n",
    "If something different happened in the last 2 weeks to the previous weeks,\n",
    "then it could break the validation set.\n",
    "For example, if there was a unique identifier which is  different in the date periods \n",
    "then we could learn to identify things using that identifier in the training set,\n",
    "But then in the last 2 weeks it may have a different set of ID's or behavior could get a lot worse.<br>\n",
    "This is not common, so it might be a bug.\n",
    "\n",
    "## Other (wrong) approaches in academia\n",
    "In industry and academic communities outside of ML, e.g., psychology, economics, etc,\n",
    "they tend to use linear/logistic regression models.\n",
    "They start with a data set and **assume the kind of parametric\n",
    "relationship between independent and dependent variable.**\n",
    "They assume that it's a linear relationship, or a linear\n",
    "relationship with a link function like a sigmoid or logistic regression.\n",
    "Assuming we know we can write this as an equation, e.g., $y = Ax_1 + Bx_2 + .... $.\n",
    "Then they find out the feature importance by looking at the order/size of the coefficients.\n",
    "(Assuming normalized data).\n",
    "\n",
    "A false common trope is that this is more accurate, pure or a better way of doing feature importance.\n",
    "If we were missing an interaction, or missing a transformation,\n",
    "or if any pre-processing was not perfect, so that the model is the absolute correct truth.\n",
    "Unless all of that is correct, the coefficients can be wrong.\n",
    "The coefficients are telling that, in the **wrong model**, this is how important those things are.\n",
    "Not helpful.\n",
    "\n",
    "The RF feature importance is telling the feature importance for an \n",
    "extremely high parameter, highly flexible functional form with few statistical assumptions. <br>\n",
    "**NB: Be skeptical when people talk about logistic regression coefficients,** \n",
    "instead of RF variable importance.\n",
    "Papers in economics, psychology, marketing, etc. (always?) have biased coefficients \n",
    "by any issues in the model.\n",
    "If they've done so much pre-processing that actually the model is pretty accurate, \n",
    "the coefficients are going to be coefficient of some principal component from a CA or a\n",
    "coefficient of some distance from some cluster, etc.\n",
    "At that point they're hard to interpret, as they're not actual variables.<br>\n",
    "Things are starting to change slowly, some fields realize this is wrong.\n",
    "But it is nearly 20 years since RF appeared so it takes a long time.\n",
    "\n",
    "## one hot encoding \n",
    "Consider a categorical variable, eg a string, \"high\", \"medium\", \"low\".\n",
    "We mapped it to numbers in the DF, so the RF doesn't know that it was\n",
    "originally a category it's just a number.\n",
    "When the RF is built, it says greater than 1, etc. <br>\n",
    "For something with, say 6 bands, maybe only one value is actually interesting, e.g. `unknown`.\n",
    "If may just happened that the way the numbers were coded, `unknown` ended up in the middle, \n",
    "then the RF will see it as a difference between 2 groups.\n",
    "That it's inefficient, wasting tree computation, \n",
    "every time we do a split we're having the amount of data at least that\n",
    "we have to do more analysis so it's going to make our tree less rich, less effective.<br>\n",
    "The data is not in a convenient way for the RF to do the work it needs to do.\n",
    "Lets create six columns, 1s and 0s, so the RF now has the ability to pick one of these \n",
    "and look at it with  one possible fit 1 vs 0.\n",
    "Now the RF can, in a single step pull out a single category level.\n",
    "This is **one-hot encoding**, and for many types of ML models this is necessary.\n",
    "\n",
    "If you are doing logistic regression, you can't possibly put in a categorical variable.\n",
    "\n",
    "We pass `max_n_cat=` parameter to `proc_df`, the max number of categories, eg 7.\n",
    "then anything with less than 7 levels is turned into a one-hot encoded set of columns.\n",
    "Try out RF and see what happens to the $R^2$ of the validation set and to the RMSE of the validation set.<br>\n",
    "We now have different features `proc_df` puts the name of the variable, an `_` and then the level name.\n",
    "Sometimes the importance of the variable changes when it is hot-encoded.\n",
    "For interpreting a model we should try one hot encoding,\n",
    "making that number as high as we can so that it doesn't take too long to compute.\n",
    "Feature importance doesn't include tiny levels that aren't interesting.\n",
    "What `enclosure_EROPS with AC` is and why is it important.\n",
    "\n",
    "### Cluster Analysis\n",
    "We saw how variables which measure the same thing confuse our\n",
    "variable importance, and also make the RF require more computation to do the same thing, etc.\n",
    "Lets do some more work to remove redundant features, using a dendrogram.\n",
    "It's a hierarchical clustering.\n",
    "Cluster analysis we look at objects (either rows in the data set or columns),\n",
    "and find which ones are similar to each other.\n",
    "In k-means we assume no labels and take a couple of data points at random.\n",
    "Iteratively we gradually find the ones that are near to it and move them closer and closer to centroids.\n",
    "\n",
    "A popular technique 20-30 years ago was \n",
    "[hierarchical clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering)\n",
    "when bottom-up is aka *agglomerated clustering*.\n",
    "In hierarchical we look at every pair of objects \n",
    "and the 2 objects that are the closest we delete and replace with their midpoint.\n",
    "We keep doing that again removing points and replacing them with their\n",
    "averages, gradually reducing a number of points by pairwise combining.\n",
    "We can plot that so rather than looking at points we look at variables, which 2 variables are the most similar.\n",
    "\n",
    "The correlation is like $R^2$ but between two variables rather than a variable and it's prediction.\n",
    "The problem with a normal correlation is that if it assumes linearity is not good.\n",
    "[Spearman's rank correlation](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient)\n",
    "is the most common rank correlation.\n",
    "It assesses how the relationship between two variables can be described using a monotonic function.\n",
    "Replace every point by its rank and then we do the same for the y-axis.\n",
    "Then we do a new plot where we plot the rank of the data.\n",
    "The rank of this data set is going to be an exact line, because every time something was greater on the x-axis\n",
    "it was also greater on the y-axis.\n",
    "If we do a correlation on the rank that's called a rank correlation. <br>\n",
    "We want to find the columns that are similar in a way that the RF would find them similar.\n",
    "RFs don't care about linearity they just care about ordering, so a rank correlation is the the right way.\n",
    "\n",
    "Once we've got a correlation matrix there's basically a couple of\n",
    "standard steps to turn that into a `dendogram`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalization\n",
    "[ML5] In ML we care about generalization accuracy, to do a good job, we need to evaluate generalizing.\n",
    "The most common way is to create a random sample, grab a few rows at random into a test set \n",
    "and build models on the rest of the rows.\n",
    "When finished, we check accuracy against the test-set to ensure that the model does generalize.\n",
    "The problem is what if it doesn't?\n",
    "We can go back, change some hyper parameters, do data augmentation, etc. to create a more generalizable model.\n",
    "And then we again check and it's still no good.\n",
    "We will keep doing this until eventually, after many attempts, it does generalize.\n",
    "But does it really generalize, or maybe we accidentally found a model which works just for that test-set?\n",
    "So we put aside a 2nd test data set. \n",
    "Otherwise, when we put it in production and it doesn't generalize that would be a bad outcome.\n",
    "\n",
    "## OOB Score\n",
    "`OOB=true` means whatever your subsample is, (a bootstrap sample or a subsample),\n",
    "take all the **other** rows and put them into into a different data set and calculate the error on those.\n",
    "It doesn't impact training, just gives an additional metric `OOB error`.\n",
    "If we don't have a validation set, this is a quasi validation set for free.\n",
    "OOB score if you don't say that it doesn't calculate it.\n",
    "If we had `set_RF_samples` pretty small compared to a big data set OOB is going to take forever to calculate.\n",
    "Hopefully we'll fix the library so that doesn't happen... \n",
    "there's no reason that need be that way but right now that's how it works.\n",
    "\n",
    "For RFs we can use the OOB score as a validation set.\n",
    "Every time we train a tree in a RF there are observations that are held out anyway \n",
    "because that's how we get some of the randomness.\n",
    "So we calculate a score for each tree based on those held out samples\n",
    "by averaging the trees that each row was not part of training. <br>\n",
    "The OOB score is similar to the validation score but on average it's less good.\n",
    "Why? Because we cannot use the whole forest, so we cannot exactly see every row.\n",
    "It uses a subset of the trees to make its less accurate prediction. <br>\n",
    "\n",
    "### Kaggle Scores\n",
    "Kaggle splits the test set into two pieces, public and private.\n",
    "When we submit predictions, a random 30% of those are used to return the leaderboard score.\n",
    "At the end of the competition, they use the other 70% to calculate the real score.\n",
    "So we can't use that feedback from the leaderboard to figure out \n",
    "some set of hyper parameters that happens to do well on the public that doesn't generalize.\n",
    "\n",
    "## Random Sample issues\n",
    "Sometimes, we are unable to use a random sample for the validation set.\n",
    "For example, because we need to maintain the temporal order in some special case.\n",
    "By using a random validation set we could get the wrong idea about a model.\n",
    "For example, if the data is imbalanced, and a random sample misses the class with few elements.\n",
    "If we are trying to predict who is going to die of lung cancer,\n",
    "and that's only .1 %, and the (random) validation set accidentally does not have\n",
    "anybody that died of lung cancer.\n",
    "\n",
    "Further, a random validation set can be plain wrong, give an inaccurate idea.\n",
    "When we build a model we always have a systematic error.\n",
    "We are going to use the model at a later time, by which time the world is different.\n",
    "When we build the model, we use old data, ie there is a time lag. \n",
    "Most of the time that matters. \n",
    "For example, predicting who is going to buy X in New Jersey.\n",
    "Say it takes 2 weeks to put the model in production, using data from the last 4 years.\n",
    "By the time it is in production, things may look very different.\n",
    "If we randomly sampled a validation set from a 4 year period, the majority of the data is over a year old.\n",
    "The X buying habits in New Jersey may have shifted, eg a recession and they now can't afford X.\n",
    "\n",
    "If we use a random sample for a validation set, we're checking how good we predict things that are obsolete now.\n",
    "Nobody cares about predicting the past. <br>\n",
    "So, anytime there's some temporary order, we select a newer part of the data as the validation set.\n",
    "We test generalization in a specific time sense: Does it generalize to the future.\n",
    "\n",
    "### Q: What about selecting the training set? \n",
    "Is it wise to take the entire full data for training or only a recent dataset? \n",
    "How do we get the validation set to be good?\n",
    "A: We build a RF on all the training data, it looks good on the training data and on the OOB.\n",
    "If it looks good on the OOB, it means we are not overfitting in a statistical sense.\n",
    "It's working well on a random sample but then it looks bad on the validation set.\n",
    "What happened was that we failed to predict the future, only predicted the past.<br>\n",
    "Maybe we shouldn't use the whole training set, we should try a recent period only.\n",
    "Con: we use less data, so we can create less rich models but Pro: it's more up-to-date data.<br>\n",
    "Most ML functions have the ability to provide a weight to each row.\n",
    "With a RF rather than bootstrapping at random we can\n",
    "have a weight on every row and randomly pick that row with some probability.\n",
    "We can pick a curve so that the most recent rows have a higher probability of being selected.\n",
    "That can work well.\n",
    "\n",
    "If we don't have a validation set that represents the future compared to what\n",
    "we are training on, we have no way to know which of the techniques are working.\n",
    "How do we make the compromise between amount of data versus recency of data.\n",
    "When we have this temporal issue, once we have something that's working well \n",
    "on the validation set, we would do the following:\n",
    "Replicate building that model again but this time we would \n",
    "combine **both** the training and validation sets together, and retrain the model.\n",
    "At that point we've got no way to test against a validation set...\n",
    "We must make sure we have a reproducible script or notebook \n",
    "that does exactly the same steps in exactly the same way.\n",
    "Because if we get something wrong then we are going to find on the test set that we've got a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensure that the validation set is representative of the test-set\n",
    "We need to know if the validation set is truly representative of the test set.\n",
    "We build 5 models on the training set, and try to have them vary in how good they are.\n",
    "We score the 5 models on the validation set and also score them on the test set.\n",
    "We are not cheating, not using any feedback from the test set to change hyper parameters.\n",
    "Only using it to check the validation set.\n",
    "We check that the 5 scores fall in a line.\n",
    "If they don't, then we are not going to get good feedback from the validation set.\n",
    "So we keep doing that process until we are  getting a line.\n",
    "That can be tricky; \n",
    "trying to create something that's as similar to the real-world is difficult.\n",
    "The same is true of creating the test-set, that also has to be as close to production as possible.\n",
    "- What's the actual mix of customers that are going to be using this?\n",
    "- How much time between when we build the model and when it is in production?\n",
    "- How often are we going to be able to refresh the model?\n",
    "These are things to think about when we build a test set.\n",
    "\n",
    "We can change the validation set, but generally we can't change the given test set.\n",
    "We start with a random sample validation set and it's all over the place.\n",
    "Then we keep tuning it (eg pick the last two months and then the first 2 weeks, etc.) \n",
    "Until we find a validation set which is indicative of our test set.\n",
    "We want some variety, how well they might generalize through time?\n",
    "So one that was trained on the whole training set, one trained on the last (2, 6) weeks, \n",
    "one which used lots of columns and might overfit a bit.\n",
    "We want to get a sense if the validation set:\n",
    "- fails to generalize temporarily \n",
    "- fails to generalize statistically.\n",
    "\n",
    "If the validation set is useful, every time the validation set score improves \n",
    "the test-set score should also improve.\n",
    "In a ML project, if we got a good test-set then, \n",
    "we know if we  screwed up something else.\n",
    "We evaluate on the test-set, and we can then try to fix it.\n",
    "But, if we screw-up creating the test-set, that is awful, \n",
    "because then we don't know if we've made a mistake.\n",
    "We build a model, test it on the test-set, it looks good, \n",
    "but the test-set was not indicative of the real-world environment.\n",
    "We should introduce models into production gradually, so we don't \"destroy the company\".\n",
    "We need to identify ahead of time when there's going to be a generalization problem.\n",
    "\n",
    "SKlearn make test trains flipped, or cross-validation functions,\n",
    "always give random samples, so most of the time we shouldn't be using them.\n",
    "RF gives us a useful OOB, but only tells that it generalizes in a statistical sense, not in a practice.\n",
    "\n",
    "## Cross-validation\n",
    "Lets explain what is **cross-validation**, and why we probably shouldn't be using it most of the time.\n",
    "Let's not just pull out one validation set but let's pull $n=5$.\n",
    "We're going to randomly shuffle the data first, then split the data into $n=5$ groups.\n",
    "For each of the 5 models we use one group as the validation set, and the rest as the training set.\n",
    "Then we take the average of the 5. <br>\n",
    "A benefit of using cross-validation over a the kind of standard validation set?\n",
    "We can use all of the data, don't have to put anything aside.\n",
    "Also we now got five models that we can ensemble together, each one used 80% of the data.\n",
    "\n",
    "#### Reasons not to use cross-validation:\n",
    "1. we have enough data so we don't not want the validations and to be included in the model trainings process\n",
    "2. longer time as we have to fit 5 models rather than one.\n",
    "3. our earlier concerns about why random validation sets are a problem are relevant here\n",
    "if we have temporal data,  there's no way to do cross validation \n",
    "We want our validation set to be as close to the test set as possible.\n",
    "So we can't do that by randomly sampling different things.\n",
    "So we do not recommend cross validation. \n",
    "It's an interesting tool to have it's easy to use.\n",
    "SKlearn has cross validation we can use sometimes but probably not often."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrapolation \n",
    "We still have a big difference between our validation score and our training score.\n",
    "We can handle the time component a little bit better.\n",
    "A problem with RFs is extrapolation.\n",
    "Say we have a sales dataset with 4 years of data.\n",
    "The RF tells us the average price over the whole training set, which can be old.\n",
    "A linear model can find a relationship between time and price, and extrapolate.\n",
    "But a RF can't do that.\n",
    "One way to address this is to avoid using time variables as predictors,\n",
    "if there's something else we can use that's better.\n",
    "Something that has a stronger relationship that's actually going to work in the future.\n",
    "\n",
    "If we understand the difference between validation and training sets,\n",
    "then that can tells what are the predictors which have a strong\n",
    "temporal component and therefore they may be irrelevant for the future time period.\n",
    "\n",
    "This is a great trick (eg in Kaggle), when they won't tell\n",
    "you whether the test set is a random sample or not.\n",
    "So we can put the test set and the training set together,\n",
    "create a new column called `is_test` and see if we can predict it.\n",
    "**If we can predict it, then the test set is not a random sample.**\n",
    "That means we have to create a validation set from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
