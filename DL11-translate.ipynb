{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq for translation\n",
    "Based on [Fastai L13](https://github.com/fastai/fastai/blob/master/courses/dl2/translate.ipynb).\n",
    "We are going to translate French into English. In this problem, the interesting part are in the architecture.\n",
    "[Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sun May  6 07:55:35 2018'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "t0 = time.clock()\n",
    "time.ctime(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "We need $(x, y)$ pairs. In this case, $x:$ French sentence, and $y:$ the equivalent English sentence.  \n",
    "We need lots of these $(x, y)$ tuples, a *“parallel corpus”*. \n",
    "For a language model, we need text. \n",
    "For translation, there are good parallel corpus available for some languages, \n",
    "e.g. the European Parliament, the UN, etc. \n",
    "For French to English, any semi-official Canadian website will have a French version and an English version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "French/English parallel texts from http://www.statmt.org/wmt15/translation-task.html, used *a set of simple heuristics to transform French URLs onto English URLs (i.e. replacing \"fr\" with \"en\" and about 40 other hand-written rules), and assume that these documents are translations of each other*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('data/translate')\n",
    "TMP_PATH = PATH/'tmp'\n",
    "TMP_PATH.mkdir(exist_ok=True)\n",
    "fname='giga-fren.release2.fixed'\n",
    "en_fname = PATH/f'{fname}.en'\n",
    "fr_fname = PATH/f'{fname}.fr'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a translation model takes a long time. To simplify [23:22], let’s only learn to translate French questions into English questions. Specifically questions that start with what/where/which/when. \n",
    "Below regex looks for things that start with “wh” and end with a question mark.\n",
    "Regular expressions are compiled into pattern objects, which have methods for various operations such as searching for pattern matches or performing string substitutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_eq = re.compile('^(Wh[^?.!]+\\?)')\n",
    "re_fq = re.compile('^([^?.!]+\\?)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We go through the corpus [23:43], open up each of the two files, each line is one parallel text, zip them together, grab the English question and the French question, and check whether they match the regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = ((re_eq.search(eq), re_fq.search(fq)) \n",
    "         for eq, fq in zip(open(en_fname, encoding='utf-8'), open(fr_fname, encoding='utf-8')))\n",
    "\n",
    "qs = [(e.group(), f.group()) for e,f in lines if e and f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 52K sentence pairs of Wh questions. Some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52331,\n",
       " [('What is light ?', 'Qu’est-ce que la lumière?'),\n",
       "  ('Who are we?', 'Où sommes-nous?'),\n",
       "  ('Where did we come from?', \"D'où venons-nous?\"),\n",
       "  ('What would we do without it?', 'Que ferions-nous sans elle ?'),\n",
       "  ('What is the absolute location (latitude and longitude) of Badger, Newfoundland and Labrador?',\n",
       "   'Quelle sont les coordonnées (latitude et longitude) de Badger, à Terre-Neuve-etLabrador?')])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qs), qs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump as a pickle so we don’t have to do it again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(qs, (PATH/'fr-en-qs.pkl').open('wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = pickle.load((PATH/'fr-en-qs.pkl').open('rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The what/who/where type questions tend to be fairly short [24:08]. \n",
    "Goal is to learn to translate arbitrary Wh questions, with no previous understanding of language.  \n",
    "50K sentences is very little data for such a complex exercise, would be impressive if we make progress. \n",
    "\n",
    "- qs contains the tuples of French and English [24:48]. \n",
    "\n",
    "Handy idiom to split them apart into 2 lists: English and French questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_qs,fr_qs = zip(*qs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the English and the French questions. \n",
    "Tokenize just means splitting them up into separate words or word-like things. \n",
    "By default [25:11], the tokenizer (a wrapper around the spaCy tokenizer) assumes English. \n",
    "For French, add an extra parameter 'fr'. \n",
    "The first time you may get an error (need spaCy French model installed).\n",
    "To download the models:<br>\n",
    "`python -m spacy download fr` <br>\n",
    "`python -m spacy download en` <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tok = Tokenizer.proc_all_mp(partition_by_cores(en_qs))\n",
    "\n",
    "fr_tok = Tokenizer.proc_all_mp(partition_by_cores(fr_qs), 'fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have RAM use problems, check `proc_all_mp()`, its processing every sentence across multiple processes [25:59]: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see the English and French tokens on a sentence.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['what', 'is', 'light', '?'],\n",
       " ['qu’', 'est', '-ce', 'que', 'la', 'lumière', '?'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_tok[0], fr_tok[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization for French is quite different: French love apostrophes and hyphens. \n",
    "An English tokenizer for a French sentence would be bad. \n",
    "Always use the right language tokenizer [28:23]. \n",
    "\n",
    "Save to disk. \n",
    "Next turn the tokens into numbers by:\n",
    "1. get a list of all of the words that appear \n",
    "2. turn every word into the index. \n",
    "\n",
    "If more than 40K words appear cut it off there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52331, 52331)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_tok), len(fr_tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see what % of the data is above some percentile.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28.0, 34.0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile([len(o) for o in en_tok], 95), np.percentile([len(o) for o in fr_tok], 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32.0, 36.0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile([len(o) for o in en_tok], 97), np.percentile([len(o) for o in fr_tok], 96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = np.array([len(o)<30 for o in en_tok])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50260, 50260)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_tok = np.array(en_tok)[keep]\n",
    "fr_tok = np.array(fr_tok)[keep]\n",
    "len(en_tok), len(fr_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(en_tok, (PATH/'en_tok.pkl').open('wb'))\n",
    "pickle.dump(fr_tok, (PATH/'fr_tok.pkl').open('wb'))\n",
    "\n",
    "en_tok = pickle.load((PATH/'en_tok.pkl').open('rb'))\n",
    "fr_tok = pickle.load((PATH/'fr_tok.pkl').open('rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We insert a few extra tokens for beginning of stream (\\_bos_), padding (\\_pad_), end of stream (\\_eos_), and unknown (\\_unk). So if we look up something that wasn’t in the 40,000 most common, then we use a `defaultdict` to return 3 which is unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def toks2ids(tok, pre):\n",
    "    freq = Counter(p for o in tok for p in o)\n",
    "    itos = [o for o,c in freq.most_common(40000)]\n",
    "    itos.insert(0, '_bos_')\n",
    "    itos.insert(1, '_pad_')\n",
    "    itos.insert(2, '_eos_')\n",
    "    itos.insert(3, '_unk')\n",
    "    stoi = collections.defaultdict(lambda: 3, {v:k for k,v in enumerate(itos)})\n",
    "    ids = np.array([([stoi[o] for o in p] + [2]) for p in tok])\n",
    "    np.save(TMP_PATH/f'{pre}_ids.npy', ids)\n",
    "    pickle.dump(itos, open(TMP_PATH/f'{pre}_itos.pkl', 'wb'))\n",
    "    return ids,itos,stoi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can every token into an ID by putting it through the string to integer dictionary (stoi) we just created and then at the end of that let’s add 2 (\\__eos__). \n",
    "[32:01]\n",
    "Now we have for later:\n",
    "- `PRE_ids` a list of IDs \n",
    "- `PRE_itos` the vocabulary \n",
    "- `PRE_stoi` reverse mapping  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17573, 24793)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_ids,en_itos,en_stoi = toks2ids(en_tok,'en')\n",
    "fr_ids,fr_itos,fr_stoi = toks2ids(fr_tok,'fr')\n",
    "len(en_itos), len(fr_itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "English (French) vocabs are 17K and 25K. Not too big. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ids(pre):\n",
    "    ids = np.load(TMP_PATH/f'{pre}_ids.npy')\n",
    "    itos = pickle.load(open(TMP_PATH/f'{pre}_itos.pkl', 'rb'))\n",
    "    stoi = collections.defaultdict(lambda: 3, {v:k for k,v in enumerate(itos)})\n",
    "    return ids,itos,stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_ids,en_itos,en_stoi = load_ids('en')\n",
    "fr_ids,fr_itos,fr_stoi = load_ids('fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To confirm it’s working, we go through each ID and convert the int to a string.\n",
    "There is the sentence, now with an \\_eos_ marker at the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['qu’', 'est', '-ce', 'que', 'la', 'lumière', '?', '_eos_'],)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[fr_itos[o] for o in fr_ids[0]], "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word vectors\n",
    "We will like to use language models... :-) But now all we have is word vectors.\n",
    "`Word2vec` is old. `fast.text` is a good source of word vectors, with hundreds of languages available.\n",
    "\n",
    "Get them from: [fasttext word vectors](https://fasttext.cc/docs/en/english-vectors.html)\n",
    "\n",
    "#!pip install git+https://github.com/facebookresearch/fastText.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastText as ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the fastText library, download the [fasttext word vectors](https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md) for your language (download the 'bin plus text' ones).<br>\n",
    "!wget https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.fr.zip <br>\n",
    "!wget https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.fr.zip <br>\n",
    "!unzip wiki.*.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vecs = ft.load_model(str((PATH/'wiki.en.bin')))\n",
    "\n",
    "fr_vecs = ft.load_model(str((PATH/'wiki.fr.bin')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above are English and French models, in both text and binary versions. \n",
    "Choose the faster binary, (text version is a bit buggy). \n",
    "Convert it to a Python dictionary. [35:55]. \n",
    "`get_vecs()` goes through each word with a dictionary comprehension and then pickle saves it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vecs(lang, ft_vecs):\n",
    "    vecd = {w:ft_vecs.get_word_vector(w) for w in ft_vecs.get_words()}\n",
    "    pickle.dump(vecd, open(PATH/f'wiki.{lang}.pkl','wb'))\n",
    "    return vecd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vecd = get_vecs('en', en_vecs)\n",
    "fr_vecd = get_vecs('fr', fr_vecs)\n",
    "\n",
    "en_vecd = pickle.load(open(PATH/'wiki.en.pkl','rb'))\n",
    "fr_vecd = pickle.load(open(PATH/'wiki.fr.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Debug code...\n",
    "ft_words = ft_vecs.get_words(include_freq=True)    #ft_vec should be en_vecd or fr_vecd\n",
    "ft_word_dict = {k:v for k,v in zip(*ft_words)}\n",
    "ft_words = sorted(ft_word_dict.keys(), key=lambda x: ft_word_dict[x])\n",
    "\n",
    "len(ft_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 300)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets look at a word, e.g. comma, and print the size of the vector\n",
    "dim_en_vec = len(en_vecd[','])\n",
    "dim_fr_vec = len(fr_vecd[','])\n",
    "dim_en_vec,dim_fr_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need the mean and standard deviation of the vectors, i.e., mean is about zero and standard deviation is about 0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0075652334, 0.29283327)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vecs = np.stack(list(en_vecd.values()))\n",
    "en_vecs.mean(),en_vecs.std()  #will need it later...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Truncate long tail\n",
    "Corpuses have a long tailed distribution of sequence length.\n",
    "The longest sequences overwhelm performance (time, RAM). \n",
    "So we grab the 99th to 97th percentile of the English and French and truncate them to that amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29, 33)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enlen_90 = int(np.percentile([len(o) for o in en_ids], 99))\n",
    "frlen_90 = int(np.percentile([len(o) for o in fr_ids], 97))\n",
    "enlen_90,frlen_90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentences up to 29 and 33 tokens long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_ids_tr = np.array([o[:enlen_90] for o in en_ids])\n",
    "fr_ids_tr = np.array([o[:frlen_90] for o in fr_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have tokenized, numericalized English and French dataset, and word vectors. \n",
    "PyTorch expects a Dataset object (ie — a length (\\__len__) and an indexer (\\__getitem__)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, x, y): self.x,self.y = x,y\n",
    "    def __getitem__(self, idx): return A(self.x[idx], self.y[idx])\n",
    "    def __len__(self): return len(self.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`A` : Numpy Arrays. It will go through each of the thing you pass it, if it is not already a numpy array, it converts into a numpy array and returns back a tuple of all of the things you passed it, \n",
    "which are now guaranteed to be numpy arrays [38:32]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training set and validation set \n",
    "Now we need to grab our English and French IDs and get a training set and a validation set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an easy way to get training and validation sets [42:45]. Get random numbers — one for each row of your data, and see if they are bigger than 0.1 or not. That gets you a list of booleans, `trn_keep`. \n",
    "Index into your array with `trn_keep` (`~trn_keep`) to grab a training (validation) set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45219, 5041)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "trn_keep = np.random.rand(len(en_ids_tr))>0.1\n",
    "en_trn,fr_trn = en_ids_tr[trn_keep],fr_ids_tr[trn_keep]\n",
    "en_val,fr_val = en_ids_tr[~trn_keep],fr_ids_tr[~trn_keep]\n",
    "len(en_trn),len(en_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create our dataset with X’s and Y’s (i.e. French and English)[43:12]. \n",
    "(To translate instead English to French, switch these two around)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_ds = Seq2SeqDataset(fr_trn,en_trn)\n",
    "val_ds = Seq2SeqDataset(fr_val,en_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create DataLoaders [43:22]. \n",
    "We can use the data loader and pass in our dataset and batch size. \n",
    "But we have to **transpose the arrays**.\n",
    "As the pre-processing is done, no need for multiple workers to do augmentation, etc.\n",
    "So `num_workers=1` saves time. \n",
    "\n",
    "Remember a tensor has to be rectangular.\n",
    "So we have to tell it what our padding index is — important because we’ve got different length sentences.\n",
    "fastai will stick them together and pad the shorter ones so that they are all equal length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampler \n",
    "[44:54] \n",
    "We have sentences of different lengths coming in. They have to be together in a mini-batch to be the same size by padding. Is better if the sentences in a mini-batch are of similar sizes. \n",
    "Otherwise they willbe as long as the longest sentence in each, wasting time and memory. \n",
    "Therefore, we use `SortSampler`, for the validation set, will sort everything by length first. \n",
    "Then for the training set, `SortishSampler` will randomize the order \n",
    "but roughly make it so that things of similar length are about in the same spot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_samp = SortishSampler(en_trn, key=lambda x: len(en_trn[x]), bs=bs)\n",
    "val_samp = SortSampler(en_val, key=lambda x: len(en_val[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dl = DataLoader(trn_ds, bs, transpose=True, transpose_y=True, num_workers=1, \n",
    "                    pad_idx=1, pre_pad=False, sampler=trn_samp)\n",
    "val_dl = DataLoader(val_ds, int(bs*1.6), transpose=True, transpose_y=True, num_workers=1, \n",
    "                    pad_idx=1, pre_pad=False, sampler=val_samp)\n",
    "md = ModelData(PATH, trn_dl, val_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the decoder we want our padding to be at the end, not at the start [44:29]:\n",
    "\n",
    "Classifier → padding in the beginning. Because we wanted that final token to represent the last word of the movie review.\n",
    "Decoder → padding at the end. As you will see, it is better to have the padding at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(31, 11), (21, 7), (21, 8), (33, 13), (33, 21)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it = iter(trn_dl)\n",
    "its = [next(it) for i in range(5)]\n",
    "[(len(x),len(y)) for x,y in its]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn-images-1.medium.com/max/1400/1*1fKDaDswwVu3w2ZtCg-Uow.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url='https://cdn-images-1.medium.com/max/1400/1*1fKDaDswwVu3w2ZtCg-Uow.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the sequence of tokens and spit them into an RNN **encoder (a.k.a. backbone)**.\n",
    "The encoder will spit out the final hidden state ($h$)which, for each sentence, it’s just a single vector.\n",
    "This part is not new [47:41]. \n",
    "\n",
    "Then $h$ goes into a $2nd$ RNN, the **decoder**. \n",
    "That is new, we need something that can go through one word at a time. \n",
    "And it keeps going until it thinks it’s finished the sentence, then it stops and returns a sentence.\n",
    "It doesn’t know how long the sentence is going to be ahead of time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s start with the encoder [48:15]. \n",
    "For variable naming, there are identical attributes for encoder and decoder. \n",
    "The encoder version has `enc`, the decoder has `dec`.\n",
    "\n",
    "- emb_enc: Embeddings for the encoder\n",
    "- gru : RNN. GRU and LSTM are nearly the same thing.\n",
    "\n",
    "We need an embedding layer because we are being passed and index of the words into a vocabulary. \n",
    "And we want to grab their fast.text embedding. \n",
    "Then over time, we might want to also fine tune to train that embedding end-to-end.\n",
    "\n",
    "`create_emb` [49:37]: \n",
    "Important to know now how to set the rows and columns for the embedding: \n",
    "the number of rows has to be equal to the vocabulary size — so each vocabulary has a word vector. \n",
    "The size of the embedding is determined by fast.text, which are size 300. \n",
    "So we use size 300, else we can’t use their embeddings.\n",
    "\n",
    "- `nn.Embedding` will initially give us a random set of embeddings [50:12]. \n",
    "We will go through each and if we find it in fast.text, \n",
    "we will replace it with the fast.text embedding. \n",
    "\n",
    "Recall that `emb.weight.data`:\n",
    "\n",
    "A PyTorch module that is learnable has a weight attribute\n",
    "A weight attribute is a Variable that has data attribute\n",
    "The data attribute is a tensor.\n",
    "Now that we’ve got our weight tensor, we can just go through our vocabulary and we can look up the word in our pre-trained vectors and if we find it, we will replace the random weights with that pre-trained vector [52:35]. \n",
    "\n",
    "The random weights have a standard deviation of 1. \n",
    "Our pre-trained vectors has a standard deviation of about 0.3. \n",
    "As a prototyping hacky, JH just multiplied it by 3 (`vecs[w] * 3$`). \n",
    "\n",
    "Some things won’t be in fast.text in which case, we’ll just keep track of it [53:22]. \n",
    "The print statement is there so that we can see what’s going on (i.e. why are we missing stuff?). \n",
    "We missed a few of the 30K... not too many."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb(vecs, itos, em_sz):\n",
    "    emb = nn.Embedding(len(itos), em_sz, padding_idx=1)\n",
    "    wgts = emb.weight.data\n",
    "    miss = []\n",
    "    for i,w in enumerate(itos):\n",
    "        try: wgts[i] = torch.from_numpy(vecs[w]*3)\n",
    "        except: miss.append(w)\n",
    "    print(len(miss),miss[5:10])\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "nh,nl = 256,2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "The encoder takes the inputs and spits out a hidden vector that (hopefully) will learn to contain all of the information about what that sentence says and how [58:49]. \n",
    "That is a pre-requisite, else the decoder can't translate...  \n",
    "So that’s what we want it to learn to do. \n",
    "As usual, we are going to do the (data, architecture, loss function) and hope.\n",
    "\n",
    "### Decoder \n",
    "[59:58]: \n",
    "Same idea, but we are going to write our own for loop. \n",
    "The for loop will exactly what the for loop inside PyTorch does for the encoder, but we are going to do it manually. \n",
    "How big is the for loop? \n",
    "It’s an output sequence length (`out_sl`) which was something passed to the constructor, \n",
    "and is equal to the length of the largest English sentence. \n",
    "Since we are translating into English, it can’t possibly be longer than that in this corpus. \n",
    "If we then used it on a longer (different) corpus, it is going to fail...\n",
    "— but we could pass in a different parameter.\n",
    "\n",
    "[1:01:06].\n",
    "We are going to go through and put it through the embedding.\n",
    "We are going to stick it through the RNN, dropout, and a linear layer.\n",
    "We will then append the output to a list which will be stacked into a single tensor and get returned.\n",
    "\n",
    "Normally, a RNN works on a whole sequence at a time, but we have a for loop to go through each part of the sequence separately [1:01:37]. \n",
    "So we have to add a leading unit axis to the start (.unsqueeze(0)) \n",
    "to say this is a sequence of length one. \n",
    "In fact, we are not really taking advantage of the RNN — we could easily re-write this with a linear layer.\n",
    "\n",
    "One thing to be aware of is `dec_inp` [1:02:34]: \n",
    "What is the input to the embedding? \n",
    "It is the previous word that we translated. \n",
    "Translating the $n$ word of a sentence without knowledge of the $n-1$ word is really hard. \n",
    "So we are going to feed that in at each time step. \n",
    "What was the previous word at the start? There was none. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically, we are going to start out with a beginning of stream token (_bos_) which is zero.\n",
    "[1:05:24]:\n",
    "- `outp`  it is a tensor whose length is equal to the number of words in the English vocabulary and it contains the probability for every one of those words that it is that word.\n",
    "\n",
    "- `outp.data.max` : It looks in its tensor to find out which word has the highest probability. \n",
    "`max()` in PyTorch returns two things: (1) the max probability and (2) the index into the array. \n",
    "We want (2), which is the word index of the largest probability.\n",
    "\n",
    "- `dec_inp` : the word index into the vocabulary. If it’s $1$ (i.e. padding), that means we are done — we reached the end. \n",
    "If it’s not $1$, let’s go back and continue.\n",
    "\n",
    "Each time, we appended our outputs (not the word but the probabilities) to the list [1:06:48] which we stack up into a tensor and we can now go ahead and feed that to a loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqRNN(nn.Module):\n",
    "    def __init__(self, vecs_enc, itos_enc, em_sz_enc, vecs_dec, itos_dec, em_sz_dec, nh, out_sl, nl=2):\n",
    "        super().__init__()\n",
    "        self.nl,self.nh,self.out_sl = nl,nh,out_sl\n",
    "        self.emb_enc = create_emb(vecs_enc, itos_enc, em_sz_enc)\n",
    "        self.emb_enc_drop = nn.Dropout(0.15)\n",
    "        self.gru_enc = nn.GRU(em_sz_enc, nh, num_layers=nl, dropout=0.25)\n",
    "        self.out_enc = nn.Linear(nh, em_sz_dec, bias=False)\n",
    "        \n",
    "        self.emb_dec = create_emb(vecs_dec, itos_dec, em_sz_dec)\n",
    "        self.gru_dec = nn.GRU(em_sz_dec, em_sz_dec, num_layers=nl, dropout=0.1)\n",
    "        self.out_drop = nn.Dropout(0.35)\n",
    "        self.out = nn.Linear(em_sz_dec, len(itos_dec))\n",
    "        self.out.weight.data = self.emb_dec.weight.data\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        sl,bs = inp.size()\n",
    "        h = self.initHidden(bs)\n",
    "        emb = self.emb_enc_drop(self.emb_enc(inp))\n",
    "        enc_out, h = self.gru_enc(emb, h)\n",
    "        h = self.out_enc(h)\n",
    "\n",
    "        dec_inp = V(torch.zeros(bs).long())\n",
    "        res = []\n",
    "        for i in range(self.out_sl):\n",
    "            emb = self.emb_dec(dec_inp).unsqueeze(0)\n",
    "            outp, h = self.gru_dec(emb, h)\n",
    "            outp = self.out(self.out_drop(outp[0]))\n",
    "            res.append(outp)\n",
    "            dec_inp = V(outp.data.max(1)[1])\n",
    "            if (dec_inp==1).all(): break\n",
    "        return torch.stack(res)\n",
    "    \n",
    "    def initHidden(self, bs): return V(torch.zeros(self.nl, bs, self.nh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function \n",
    "[1:07:13] The loss function is categorical cross entropy loss. \n",
    "We have a list of probabilities for each of our classes. \n",
    "The classes are all the words in our English vocab.\n",
    "The target is the correct class (i.e. which is the correct word at this location). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_loss(input, target):\n",
    "    sl,bs = target.size()\n",
    "    sl_in,bs_in,nc = input.size()\n",
    "    if sl>sl_in: input = F.pad(input, (0,0,0,0,0,sl-sl_in))\n",
    "    input = input[:sl]\n",
    "    return F.cross_entropy(input.view(-1,nc), target.view(-1))#, ignore_index=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there are 2 Tweaks, so we had to write our own loss function. (basically it is cross entropy loss).\n",
    "[1:07:40]:\n",
    "\n",
    "1. If the generated sequence length is shorter than the sequence length of the target, we need to add some padding. \n",
    "PyTorch padding function requires a tuple of 6 to pad a rank 3 tensor (sequence length, batch size, by number of words in the vocab). Each pair represents padding before and after that dimension.\n",
    "2. `F.cross_entropy` expects a rank 2 tensor, but we have sequence length by batch size, so just flatten out. That is what view(-1, ...) does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between .cuda() and to_gpu() : to_gpu will not put to in the GPU if you do not have one. You can also set fastai.core.USE_GPU to false to force it to not use GPU that can be handy for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3097 ['l’', \"d'\", 't_up', 'd’', \"qu'\"]\n",
      "1285 [\"'s\", '’s', \"n't\", 'n’t', ':']\n"
     ]
    }
   ],
   "source": [
    "rnn = Seq2SeqRNN(fr_vecd, fr_itos, dim_fr_vec, en_vecd, en_itos, dim_en_vec, nh, enlen_90)\n",
    "learn = RNN_Learner(md, SingleModel(to_gpu(rnn)), opt_fn=opt_fn)\n",
    "learn.crit = seq2seq_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need something that tells it how to handle learning rate groups. `SingleModel` that you can pass it to which treats the whole thing as a single learning rate group [1:09:40]. That is the easiest way to turn a PyTorch module into a fastai model.\n",
    "\n",
    "We could just call Learner to turn that into a learner, but if we call RNN_Learner, it does add in save_encoder and load_encoder that can be handy sometimes. \n",
    "In this case, we really could have said Learner but RNN_Learner also works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fa3bd0e9ce047ef9e4e2da7fa7edda6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 238/362 [00:32<00:17,  7.23it/s, loss=32.3]"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()\n",
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=3e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dbede9ab681453cad630376fe43b48a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=15), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 46/362 [00:06<00:43,  7.29it/s, loss=8.74]\n",
      "epoch      trn_loss   val_loss                              \n",
      "    0      5.160235   5.583373  \n",
      "    1      4.510187   4.633612                              \n",
      "    2      4.013664   4.261124                              \n",
      "    3      3.708543   3.979783                              \n",
      "    4      3.512268   3.800645                              \n",
      "    5      3.425806   3.705089                              \n",
      "    6      3.068225   3.633511                              \n",
      "    7      3.010568   3.623787                              \n",
      "    8      3.034442   3.54212                               \n",
      "    9      3.088963   3.600699                              \n",
      "    10     2.730439   3.527829                              \n",
      "    11     2.720195   3.506983                              \n",
      "    12     2.663137   3.492394                              \n",
      "    13     2.530791   3.482845                              \n",
      "    14     2.573224   3.501848                              \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cle1 = learn.fit(lr, 1, cycle_len=15, use_clr=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('initial')\n",
    "\n",
    "learn.load('initial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [1:11:01]\n",
    "Remember the model attribute of a learner is a standard PyTorch model so we can pass some $x$ which we can grab out of our validation set. \n",
    "Or you could `learn.predict_array` to get some predictions. \n",
    "Then we convert those predictions into words by going .max()[1] to grab the index of the highest probability words to get some predictions. \n",
    "Then for a few examples lets print the French, the correct English, and the predicted English,\n",
    "for things that are not padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "que considérez - vous comme les lacunes / besoins clés transversaux et dans des domaines précis de la recherche en hygiène de l’ environnement au canada ? _eos_\n",
      "what do you consider to be the key issue - specific and cross - cutting gaps / needs in environmental health research in canada ? _eos_\n",
      "what do you see the the key areas in the in in in health and and health canada canada ? ? _eos_ _eos_ _eos_\n",
      "\n",
      "que faire si le crédit pour la t_up tps / t_up tvh de l’ époux ou conjoint de fait survivant comprend un montant pour la personne décédée ? _eos_\n",
      "what if the surviving spouse ’s or common - law partner ’s t_up gst / t_up hst credit includes a claim for the deceased ? _eos_\n",
      "what if the deceased gst the t_up gst / / t_up t_up t_up t_up spouse t_up t_up t_up partner partner ? ? ? ? _eos_\n",
      "\n",
      "quelle est la validité d' un paiement qui est soit dérisoire , soit sans rapport avec le coût de production du médicament offert comme échantillon ? _eos_\n",
      "what is the validity of a payment which is either insignificant or not related to the production cost of the drug \" sampled \" ? _eos_\n",
      "what is the actual of of payment a , , , , , , , cost cost ? ? ? ? _eos_ _eos_\n",
      "\n",
      "quelles mesures de protection ont été prises dans le but de protéger l' accès à l' information et l' intégrité des données au cours de l' entretien périodique ou de l' entretien du\n",
      "what safeguards have been implemented to protect access to the information and the data integrity during periodic maintenance , or servicing of the server ? _eos_\n",
      "what protection have have been to protect the the the the the the and and and and or or or ? ? ? ? _eos_\n",
      "\n",
      "les exigences proposées des bonnes pratiques de fabrication sont - elles significatives et suffisantes afin de protéger les intérêts des consommateurs ? _eos_\n",
      "what is a reasonable period of time to allow manufacturers , importers and distributors and packagers of natural health products to meet these requirements ? _eos_\n",
      "what are the best practices to to to to to to and and interests interests ? ? _eos_ _eos_\n",
      "\n",
      "de quels autres facteurs doit - on tenir compte lorsque l' on se penche sur l' acceptabilité d' instruments médicaux contenant du t_up dehp dans le contexte de traitements médicaux particuliers ? _eos_\n",
      "what other factors need to be taken into account in reviewing the acceptability of t_up dehp - containing medical devices for specific medical treatments ? _eos_\n",
      "what other factors should consider consider consider consider consider medical medical medical medical medical for medical medical medical medical for for medical ? ? ? ? _eos_ _eos_ _eos_\n",
      "\n",
      "quel est ( ou quel sera ) l' âge des animaux utilisés pour assurer l' approvisionnement des ingrédients énumérés aux sections 3 et 4 ? _eos_\n",
      "what is ( or will be ) the age of the animal(s ) used to source the ingredients listed in sections 3 and 4 ? _eos_\n",
      "what is the be used to the the the the the the the the the the the the the the ) ? ? ? _eos_ _eos_\n",
      "\n",
      "tableau des corrélations entre les questions no 13 ( à votre avis , quels sont les clients ou les groupes d' utilisateurs les plus aptes à profiter d' un guide national sur le\n",
      "who do you think are the most important client or user groups for a national guide on the role of health professionals in ea ? _eos_\n",
      "what issues you you you you you the the the the the the the the the the the the the guide guide ? ? ? ?\n",
      "\n",
      "que se passe -t -il si un groupe ou une organisation ne satisfait pas aux exigences de confidentialité énoncées dans les accords de contribution de la direction générale de la santé des premières\n",
      "what happens if a group or organization does not meet the first nations inuit health branch ( t_up fnihb ) contribution agreement confidentiality requirements ? _eos_\n",
      "what happens if a organization organization or or or organization the the the the the the the the the the the the ? ? _eos_ _eos_\n",
      "\n",
      "quels changements ont - ils été apportés ou sont - ils envisagés au niveau des politiques , des systèmes et(ou ) du milieu de travail pour améliorer les conditions du milieu de travail\n",
      "what changes have been made or are being considered at the policy , system , and / or workplace levels to improve workplace conditions ? _eos_\n",
      "what changes were made , or , , , , the to to work to work to work conditions work work ? ? _eos_ _eos_\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x,y = next(iter(val_dl))\n",
    "probs = learn.model(V(x))\n",
    "preds = to_np(probs.max(2)[1])\n",
    "\n",
    "for i in range(170,180):\n",
    "    print(' '.join([fr_itos[o] for o in x[:,i] if o != 1]))\n",
    "    print(' '.join([en_itos[o] for o in y[:,i] if o != 1]))\n",
    "    print(' '.join([en_itos[o] for o in preds[:,i] if o!=1]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB:** a simple (possible written largely from scratch) PyTorch module, on only 50K sentences is sometimes capable, on a validation set, of giving the right answer. \n",
    "Not perfect, sometimes slightly different wording, \n",
    "and sometimes sentences that aren’t grammatically sensible, or have too many question marks. \n",
    "But we are on the right track. \n",
    "Even the simplest possible seq-to-seq trained for a small number of epochs without any pre-training \n",
    "(other than the use of word embeddings) is surprisingly good. \n",
    "\n",
    "So a simple seq-to-seq model with little data is surprisingly effective, and in certain situations this may be enough for our needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trick #1 : Go bi-directional\n",
    "For classification, the approach to bi-directional is take all of your token sequences, spin them around, train a new language model, and train a new classifier. \n",
    "JH mentioned that in the wikitext pre-trained model if you replace fwd with bwd in the name, you will get the pre-trained backward model he created for you. \n",
    "Get a set of predictions and then average the predictions just like a normal ensemble. \n",
    "That is how we do bi-dir for that kind of classification. \n",
    "There may be ways to do it end-to-end, but JH hasn’t quite figured them out, so they are not in fastai yet. \n",
    "So if you figure it out, that’s an interesting line of research. \n",
    "\n",
    "But because we are not doing massive documents where we have to chunk it into separate bits and then pool over them, we can do bi-dir very easily in this case. \n",
    "It is literally as simple as adding bidirectional=True to our encoder. \n",
    "People tend not to do bi-dir for the decoder partly because it is *kind of considered cheating*...\n",
    "But maybe it can work in some situations although it might need to be more of an ensemble approach in the decoder because it’s a bit less obvious. \n",
    "\n",
    "But encoder is simple — `bidirectional=True` and we have a $2nd$ RNN that is going in the opposite direction.\n",
    "The $2nd$ RNN is visiting each token in the opposing order, so when we get to the final hidden state, it is the first (i.e. left most) token. \n",
    "But the hidden state is the same size, so the final result is that we end up with a tensor with an extra axis of length 2. \n",
    "\n",
    "NB: Depending on the library, often that will be then combined with the number of layers, \n",
    "so if you have 2 layers and bi-directional — that tensor dimension is now length 4. \n",
    "With PyTorch it depends which bit of the process you are looking at as to whether you get a separate result for each layer and/or for each bidirectional bit. \n",
    "The documentation will tell the input’s output’s tensor sizes appropriate for the number of layers and whether you have bidirectional=True.\n",
    "\n",
    "In this particular case, you will see all the changes that had to be made [1:19:38]. \n",
    "For example, when we added `bidirectional=True`, the Linear layer now needs number of hidden times 2 (i.e. $nh*2$) to reflect that we have that $2nd$ direction in our hidden state. Also in `initHidden` it’s now $self.nl*2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqRNN_Bidir(nn.Module):\n",
    "    def __init__(self, vecs_enc, itos_enc, em_sz_enc, vecs_dec, itos_dec, em_sz_dec, nh, out_sl, nl=2):\n",
    "        super().__init__()\n",
    "        self.emb_enc = create_emb(vecs_enc, itos_enc, em_sz_enc)\n",
    "        self.nl,self.nh,self.out_sl = nl,nh,out_sl\n",
    "        self.gru_enc = nn.GRU(em_sz_enc, nh, num_layers=nl, dropout=0.25, bidirectional=True)\n",
    "        self.out_enc = nn.Linear(nh*2, em_sz_dec, bias=False)\n",
    "        self.drop_enc = nn.Dropout(0.05)\n",
    "        self.emb_dec = create_emb(vecs_dec, itos_dec, em_sz_dec)\n",
    "        self.gru_dec = nn.GRU(em_sz_dec, em_sz_dec, num_layers=nl, dropout=0.1)\n",
    "        self.emb_enc_drop = nn.Dropout(0.15)\n",
    "        self.out_drop = nn.Dropout(0.35)\n",
    "        self.out = nn.Linear(em_sz_dec, len(itos_dec))\n",
    "        self.out.weight.data = self.emb_dec.weight.data\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        sl,bs = inp.size()\n",
    "        h = self.initHidden(bs)\n",
    "        emb = self.emb_enc_drop(self.emb_enc(inp))\n",
    "        enc_out, h = self.gru_enc(emb, h)\n",
    "        h = h.view(2,2,bs,-1).permute(0,2,1,3).contiguous().view(2,bs,-1)\n",
    "        h = self.out_enc(self.drop_enc(h))\n",
    "\n",
    "        dec_inp = V(torch.zeros(bs).long())\n",
    "        res = []\n",
    "        for i in range(self.out_sl):\n",
    "            emb = self.emb_dec(dec_inp).unsqueeze(0)\n",
    "            outp, h = self.gru_dec(emb, h)\n",
    "            outp = self.out(self.out_drop(outp[0]))\n",
    "            res.append(outp)\n",
    "            dec_inp = V(outp.data.max(1)[1])\n",
    "            if (dec_inp==1).all(): break\n",
    "        return torch.stack(res)\n",
    "    \n",
    "    def initHidden(self, bs): return V(torch.zeros(self.nl*2, bs, self.nh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Why use a range to the loop? [1:20:58] \n",
    "```\n",
    "for i in range(self.out_sl):\n",
    "```\n",
    "Because when we start training, everything is random so if \n",
    "`(dec_inp==1).all(): break` will probably never be true. \n",
    "Later on, it will always break, so this is like \"forever loop until break\". \n",
    "NB: At the start, the model knows nothing. \n",
    "So we must make sure that it will do something at least vaguely sensible....\n",
    "See Teacher forcing for this...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3097 ['l’', \"d'\", 't_up', 'd’', \"qu'\"]\n",
      "1285 [\"'s\", '’s', \"n't\", 'n’t', ':']\n"
     ]
    }
   ],
   "source": [
    "rnn = Seq2SeqRNN_Bidir(fr_vecd, fr_itos, dim_fr_vec, en_vecd, en_itos, dim_en_vec, nh, enlen_90)\n",
    "learn = RNN_Learner(md, SingleModel(to_gpu(rnn)), opt_fn=opt_fn)\n",
    "learn.crit = seq2seq_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d5b6d9e6cce4522b83a180273246b22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                              \n",
      "    0      4.578465   4.351237  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cle2 = learn.fit(lr, 1, cycle_len=1, use_clr=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "cle1": {},
     "cle2": {}
    }
   },
   "source": [
    "We got {{cle1}} cross entropy loss with single direction [1:21:46], and with bi-direction, {{cle2}}, improved a little. It shouldn’t really slow things down too much. \n",
    "Bi-directional does mean there is a little bit more sequential processing have to happen, but it is generally a good win. \n",
    "In the Google translation model, of the 8 layers, only the first layer is bi-directional, because it allows it to do more in parallel. \n",
    "If we create really deep models, think about which ones are bi-directional otherwise we will have performance issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('bidir')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teacher forcing\n",
    "When a model starts learning, it knows nothing, so it is going to spit out meaningless words,\n",
    "which are not helpful for the next process as an input.  \n",
    "Early learning is difficult because it is feeding in an input that is stupid into a model that knows nothing. \n",
    "So it is not asking too much eventually it gets there, but it’s definitely not as helpful as we can be. \n",
    "Instead, in teacher forcing, we feed in the actual correct word. \n",
    "(Of course, we can’t do that at inference time, only during training).\n",
    "\n",
    "`pr_force` is the probability of forcing [1:24:01]. \n",
    "If some random number is less than that probability then we replace our decoder input with the correct (ground truth) word. \n",
    "If we have already gone too far and if it is already longer than the target sequence, we are just going to stop. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s now write something such that in the training loop, it gradually decreases `pr_force` [1:26:01] \n",
    "But not from scratch! The fastai training loop already provides convinient things like:\n",
    "- progress bars, \n",
    "- exponential weighted averages to smooth out the losses, \n",
    "- keeps track of metrics,  \n",
    "- keep track of calling the reset for RNN at the start of the epoch to make sure the hidden state is set to zeros. \n",
    "\n",
    "So lets just add a \"hook\", in this case, the `stepper`. model.py is where the `fit` function lives, the lowest level thing that does not require learner — just requires a standard PyTorch model and a model data object. \n",
    "You just need to know how many epochs, a standard PyTorch optimizer, and a standard PyTorch loss function. We hardly ever used in the class, we normally call learn.fit, (and learn.fit calls fit).<br>\n",
    "`stepper.step` is responsible for:\n",
    "- calling the model\n",
    "- getting the loss\n",
    "- finding the loss function\n",
    "- calling the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqStepper(Stepper):\n",
    "    def step(self, xs, y, epoch):\n",
    "        self.m.pr_force = (10-epoch)*0.1 if epoch<10 else 0\n",
    "        xtra = []\n",
    "        output = self.m(*xs, y)\n",
    "        if isinstance(output,tuple): output,*xtra = output\n",
    "        self.opt.zero_grad()\n",
    "        loss = raw_loss = self.crit(output, y)\n",
    "        if self.reg_fn: loss = self.reg_fn(output, xtra, raw_loss)\n",
    "        loss.backward()\n",
    "        if self.clip:   # Gradient clipping\n",
    "            nn.utils.clip_grad_norm(trainable_params_(self.m), self.clip)\n",
    "        self.opt.step()\n",
    "        return raw_loss.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JH suspects that the above code is too complicated... Probably could have been just:\n",
    "```\n",
    "    class Seq2SeqStepper(Stepper):\n",
    "    def step(self, xs, y, epoch):\n",
    "        self.m.pr_force = (10-epoch)*0.1 if epoch<10 else 0\n",
    "        return super.step(xs, y, epoch)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1:25:29]. At the start of training, let’s set `pr_force` high so that nearly always it gets the actual correct previous word. As we train, we decrease `pr_force` all the way to $0$ (here at epoch 10). \n",
    "Then it has to learn properly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqRNN_TeacherForcing(nn.Module):\n",
    "    def __init__(self, vecs_enc, itos_enc, em_sz_enc, vecs_dec, itos_dec, em_sz_dec, nh, out_sl, nl=2):\n",
    "        super().__init__()\n",
    "        self.emb_enc = create_emb(vecs_enc, itos_enc, em_sz_enc)\n",
    "        self.nl,self.nh,self.out_sl = nl,nh,out_sl\n",
    "        self.gru_enc = nn.GRU(em_sz_enc, nh, num_layers=nl, dropout=0.25)\n",
    "        self.out_enc = nn.Linear(nh, em_sz_dec, bias=False)\n",
    "        self.emb_dec = create_emb(vecs_dec, itos_dec, em_sz_dec)\n",
    "        self.gru_dec = nn.GRU(em_sz_dec, em_sz_dec, num_layers=nl, dropout=0.1)\n",
    "        self.emb_enc_drop = nn.Dropout(0.15)\n",
    "        self.out_drop = nn.Dropout(0.35)\n",
    "        self.out = nn.Linear(em_sz_dec, len(itos_dec))\n",
    "        self.out.weight.data = self.emb_dec.weight.data\n",
    "        self.pr_force = 1.\n",
    "        \n",
    "    def forward(self, inp, y=None):\n",
    "        sl,bs = inp.size()\n",
    "        h = self.initHidden(bs)\n",
    "        emb = self.emb_enc_drop(self.emb_enc(inp))\n",
    "        enc_out, h = self.gru_enc(emb, h)\n",
    "        h = self.out_enc(h)\n",
    "\n",
    "        dec_inp = V(torch.zeros(bs).long())\n",
    "        res = []\n",
    "        for i in range(self.out_sl):\n",
    "            emb = self.emb_dec(dec_inp).unsqueeze(0)\n",
    "            outp, h = self.gru_dec(emb, h)\n",
    "            outp = self.out(self.out_drop(outp[0]))\n",
    "            res.append(outp)\n",
    "            dec_inp = V(outp.data.max(1)[1])\n",
    "            if (dec_inp==1).all(): break\n",
    "            if (y is not None) and (random.random()<self.pr_force):\n",
    "                if i>=len(y): break\n",
    "                dec_inp = y[i]\n",
    "        return torch.stack(res)\n",
    "    \n",
    "    def initHidden(self, bs): return V(torch.zeros(self.nl, bs, self.nh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following 3 lines were added:\n",
    "```\n",
    "    if (y is not None) and (random.random()<self.pr_force):\n",
    "                if i>=len(y): break\n",
    "                dec_inp = y[i]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = Seq2SeqRNN_TeacherForcing(fr_vecd, fr_itos, dim_fr_vec, en_vecd, en_itos, dim_en_vec, nh, enlen_90)\n",
    "learn = RNN_Learner(md, SingleModel(to_gpu(rnn)), opt_fn=opt_fn)\n",
    "learn.crit = seq2seq_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At least 11 epochs as before then it was \"cheating\" by getting the right value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cle3 = learn.fit(lr, 1, cycle_len=12, use_clr=(20,10), stepper=Seq2SeqStepper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('forcing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attentional model\n",
    "Attention is cool. \n",
    "Expecting the entirety of the sentence to be summarized into this single hidden vector is asking a lot. \n",
    "It has to know what was said, how it was said, and everything necessary to create the sentence in the 2nd language.  \n",
    "\n",
    "Lets use a model where we output a hidden state **after every single word**. \n",
    "That information (hidden states) is already there but we’ve just been throwing it away. \n",
    "Not only that but bi-directional, we got two vectors of state every step that we can use. <br>\n",
    "How can we do this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_t(*sz): return torch.randn(sz)/math.sqrt(sz[0])\n",
    "def rand_p(*sz): return nn.Parameter(rand_t(*sz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqAttnRNN(nn.Module):\n",
    "    def __init__(self, vecs_enc, itos_enc, em_sz_enc, vecs_dec, itos_dec, em_sz_dec, nh, out_sl, nl=2):\n",
    "        super().__init__()\n",
    "        self.emb_enc = create_emb(vecs_enc, itos_enc, em_sz_enc)\n",
    "        self.nl,self.nh,self.out_sl = nl,nh,out_sl\n",
    "        self.gru_enc = nn.GRU(em_sz_enc, nh, num_layers=nl, dropout=0.25)\n",
    "        self.out_enc = nn.Linear(nh, em_sz_dec, bias=False)\n",
    "        self.emb_dec = create_emb(vecs_dec, itos_dec, em_sz_dec)\n",
    "        self.gru_dec = nn.GRU(em_sz_dec, em_sz_dec, num_layers=nl, dropout=0.1)\n",
    "        self.emb_enc_drop = nn.Dropout(0.15)\n",
    "        self.out_drop = nn.Dropout(0.35)\n",
    "        self.out = nn.Linear(em_sz_dec*2, len(itos_dec))\n",
    "        self.out.weight.data = self.emb_dec.weight.data\n",
    "\n",
    "        self.W1 = rand_p(nh, em_sz_dec)\n",
    "        self.l2 = nn.Linear(em_sz_dec, em_sz_dec)\n",
    "        self.l3 = nn.Linear(em_sz_dec+nh, em_sz_dec)\n",
    "        self.V = rand_p(em_sz_dec)\n",
    "\n",
    "    def forward(self, inp, y=None, ret_attn=False):\n",
    "        sl,bs = inp.size()\n",
    "        h = self.initHidden(bs)\n",
    "        emb = self.emb_enc_drop(self.emb_enc(inp))\n",
    "        enc_out, h = self.gru_enc(emb, h)\n",
    "        h = self.out_enc(h)\n",
    "\n",
    "        dec_inp = V(torch.zeros(bs).long())\n",
    "        res,attns = [],[]\n",
    "        w1e = enc_out @ self.W1\n",
    "        for i in range(self.out_sl):\n",
    "            w2h = self.l2(h[-1])\n",
    "            u = F.tanh(w1e + w2h)\n",
    "            a = F.softmax(u @ self.V, 0)\n",
    "            attns.append(a)\n",
    "            Xa = (a.unsqueeze(2) * enc_out).sum(0)\n",
    "            emb = self.emb_dec(dec_inp)\n",
    "            wgt_enc = self.l3(torch.cat([emb, Xa], 1))\n",
    "            \n",
    "            outp, h = self.gru_dec(wgt_enc.unsqueeze(0), h)\n",
    "            outp = self.out(self.out_drop(outp[0]))\n",
    "            res.append(outp)\n",
    "            dec_inp = V(outp.data.max(1)[1])\n",
    "            if (dec_inp==1).all(): break\n",
    "            if (y is not None) and (random.random()<self.pr_force):\n",
    "                if i>=len(y): break\n",
    "                dec_inp = y[i]\n",
    "\n",
    "        res = torch.stack(res)\n",
    "        if ret_attn: res = res,torch.stack(attns)\n",
    "        return res\n",
    "\n",
    "    def initHidden(self, bs): return V(torch.zeros(self.nl, bs, self.nh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With attention, most of the code is identical. The one major difference is this line: <br>\n",
    "```\n",
    "Xa = (a.unsqueeze(2) * enc_out).sum(0) \n",
    "```. \n",
    "We are going to take a weighted average, by means of the little neural net below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = Seq2SeqAttnRNN(fr_vecd, fr_itos, dim_fr_vec, en_vecd, en_itos, dim_en_vec, nh, enlen_90)\n",
    "learn = RNN_Learner(md, SingleModel(to_gpu(rnn)), opt_fn=opt_fn)\n",
    "learn.crit = seq2seq_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use softmax to ensure all of the weights add up to 1, and expect that one of those weights should probably be higher than the other ones [1:36:38]. Softmax guarantees that they add up to 1, and it tends to encourage one of the weights to be higher than the others.\n",
    "\n",
    "[1:37:09]. We take the last layer’s hidden state and stick it into a linear layer (`l2`). \n",
    "Then stick it into a nonlinear activation `F.tanh`, then do a matrix multiply. \n",
    "So it is a NN with one hidden layer: a linear layer, nonlinear activation, matrix multiplication. \n",
    "Stick it into a softmax and then use that to weight our encoder outputs. \n",
    "Now rather than just taking the last encoder output, we have the whole tensor of all of the encoder outputs which we just weight by this NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=2e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cle4 = learn.fit(lr, 1, cycle_len=15, use_clr=(20,10), stepper=Seq2SeqStepper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "cle4": {}
    }
   },
   "source": [
    "And now our loss is down to {{cle4}}. We needed to make sure at least do 10 epochs because before that, it was cheating by using the teacher forcing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('attn')\n",
    "\n",
    "learn.load('attn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teacher forcing had 3.49 and now with nearly exactly the same thing but we’ve got a minimal NN figuring out what weightings to give our inputs and we are down to 3.27. \n",
    "These loss are logs, so $e^{3.27}$ is quite a significant change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(val_dl))\n",
    "probs,attns = learn.model(V(x),ret_attn=True)\n",
    "preds = to_np(probs.max(2)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(140,150):\n",
    "    print(' '.join([fr_itos[o] for o in x[:,i] if o != 1]))\n",
    "    print(' '.join([en_itos[o] for o in y[:,i] if o != 1]))\n",
    "    print(' '.join([en_itos[o] for o in preds[:,i] if o!=1]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization [1:47:12]\n",
    "We can grab the attentions out of the model by adding return attention parameter to forward function. \n",
    "(You can put anything in a forward function argument). \n",
    "So we added a return attention parameter, false by default (obviously the training loop doesn’t know anything about it). \n",
    "If the parameter is True, then stick the attentions on <br>\n",
    "`if ret_attn: res = res,torch.stack(attns))`. <br>\n",
    "The attentions is simply the value `a` just chuck it on a list (`attns.append(a)`). \n",
    "We call the model with `ret_attn = True` and get back both the probabilities and the attentions [1:47:53]:\n",
    "We can now draw pictures, at each time step, of the attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = to_np(attns[...,180])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(15, 10))\n",
    "for i,ax in enumerate(axes.flat):\n",
    "    ax.plot(attn[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While building something like this, we don’t really know why it’s not working right.\n",
    "Early attempts were broken, it wasn’t really learning anything useful. \n",
    "It was giving equal attention to everything and it wasn’t worse — it just wasn’t much better. \n",
    "When you visualize in a way that you know what it ought to look like ahead of time, \n",
    "you don’t really know if it’s working [1:49:16]. \n",
    "**NB: is important to find ways to check your intermediate steps in your outputs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqRNN_All(nn.Module):\n",
    "    def __init__(self, vecs_enc, itos_enc, em_sz_enc, vecs_dec, itos_dec, em_sz_dec, nh, out_sl, nl=2):\n",
    "        super().__init__()\n",
    "        self.emb_enc = create_emb(vecs_enc, itos_enc, em_sz_enc)\n",
    "        self.nl,self.nh,self.out_sl = nl,nh,out_sl\n",
    "        self.gru_enc = nn.GRU(em_sz_enc, nh, num_layers=nl, dropout=0.25, bidirectional=True)\n",
    "        self.out_enc = nn.Linear(nh*2, em_sz_dec, bias=False)\n",
    "        self.drop_enc = nn.Dropout(0.25)\n",
    "        self.emb_dec = create_emb(vecs_dec, itos_dec, em_sz_dec)\n",
    "        self.gru_dec = nn.GRU(em_sz_dec, em_sz_dec, num_layers=nl, dropout=0.1)\n",
    "        self.emb_enc_drop = nn.Dropout(0.15)\n",
    "        self.out_drop = nn.Dropout(0.35)\n",
    "        self.out = nn.Linear(em_sz_dec, len(itos_dec))\n",
    "        self.out.weight.data = self.emb_dec.weight.data\n",
    "\n",
    "        self.W1 = rand_p(nh*2, em_sz_dec)\n",
    "        self.l2 = nn.Linear(em_sz_dec, em_sz_dec)\n",
    "        self.l3 = nn.Linear(em_sz_dec+nh*2, em_sz_dec)\n",
    "        self.V = rand_p(em_sz_dec)\n",
    "\n",
    "    def forward(self, inp, y=None):\n",
    "        sl,bs = inp.size()\n",
    "        h = self.initHidden(bs)\n",
    "        emb = self.emb_enc_drop(self.emb_enc(inp))\n",
    "        enc_out, h = self.gru_enc(emb, h)\n",
    "        h = h.view(2,2,bs,-1).permute(0,2,1,3).contiguous().view(2,bs,-1)\n",
    "        h = self.out_enc(self.drop_enc(h))\n",
    "\n",
    "        dec_inp = V(torch.zeros(bs).long())\n",
    "        res,attns = [],[]\n",
    "        w1e = enc_out @ self.W1\n",
    "        for i in range(self.out_sl):\n",
    "            w2h = self.l2(h[-1])\n",
    "            u = F.tanh(w1e + w2h)\n",
    "            a = F.softmax(u @ self.V, 0)\n",
    "            attns.append(a)\n",
    "            Xa = (a.unsqueeze(2) * enc_out).sum(0)\n",
    "            emb = self.emb_dec(dec_inp)\n",
    "            wgt_enc = self.l3(torch.cat([emb, Xa], 1))\n",
    "            \n",
    "            outp, h = self.gru_dec(wgt_enc.unsqueeze(0), h)\n",
    "            outp = self.out(self.out_drop(outp[0]))\n",
    "            res.append(outp)\n",
    "            dec_inp = V(outp.data.max(1)[1])\n",
    "            if (dec_inp==1).all(): break\n",
    "            if (y is not None) and (random.random()<self.pr_force):\n",
    "                if i>=len(y): break\n",
    "                dec_inp = y[i]\n",
    "        return torch.stack(res)\n",
    "\n",
    "    def initHidden(self, bs): return V(torch.zeros(self.nl*2, bs, self.nh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = Seq2SeqRNN_All(fr_vecd, fr_itos, dim_fr_vec, en_vecd, en_itos, dim_en_vec, nh, enlen_90)\n",
    "learn = RNN_Learner(md, SingleModel(to_gpu(rnn)), opt_fn=opt_fn)\n",
    "learn.crit = seq2seq_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cle5 = learn.fit(lr, 1, cycle_len=15, use_clr=(20,10), stepper=Seq2SeqStepper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(val_dl))\n",
    "probs = learn.model(V(x))\n",
    "preds = to_np(probs.max(2)[1])\n",
    "\n",
    "for i in range(180,190):\n",
    "    print(' '.join([fr_itos[o] for o in x[:,i] if o != 1]))\n",
    "    print(' '.join([en_itos[o] for o in y[:,i] if o != 1]))\n",
    "    print(' '.join([en_itos[o] for o in preds[:,i] if o!=1]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.clock()\n",
    "time.ctime(time.time())\n",
    "t1-t0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {
    "height": "253px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
